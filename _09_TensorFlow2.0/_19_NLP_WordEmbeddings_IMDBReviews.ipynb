{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demonstrate Word Embeddings with IMDB movie reviews"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 12:16:35.170691: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB movie reviews dataset\n",
    "\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 12:16:35.394164: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract the relevant data lists from the training and test data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data , test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "# For trainings , our labels are expected to be numpy arrays\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize our sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_token = '<OOV>'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Similarly for testing sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? a very close and sharp <OOV> of the <OOV> and dynamic emotional world of <OOV> one <OOV> old guy that makes his first experiences in his gay love to an other boy during an vacation with a part of his family br br i liked this film because of his <OOV> clear and <OOV> storytelling with all this sound close ups and quiet moments <OOV> had been full of <OOV> <OOV> br br\n",
      "A very close and sharp discription of the bubbling and dynamic emotional world of specialy one 18year old guy, that makes his first experiences in his gay love to an other boy, during an vacation with a part of his family.<br /><br />I liked this film because of his extremly clear and surrogated storytelling , with all this \"Sound-close-ups\" and quiet moments wich had been full of intensive moods.<br /><br />\n"
     ]
    }
   ],
   "source": [
    "# Lets check one sentence - original and decoded from padding\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[3]))\n",
    "print(training_sentences[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we will build and train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5981 - accuracy: 0.6403 - val_loss: 0.3393 - val_accuracy: 0.8521\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2359 - accuracy: 0.9121 - val_loss: 0.3675 - val_accuracy: 0.8404\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0976 - accuracy: 0.9762 - val_loss: 0.4681 - val_accuracy: 0.8228\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0247 - accuracy: 0.9972 - val_loss: 0.5238 - val_accuracy: 0.8273\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0068 - accuracy: 0.9995 - val_loss: 0.5889 - val_accuracy: 0.8281\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 9.6001e-04 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.8288\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 5.3015e-04 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.9390e-04 - accuracy: 1.0000 - val_loss: 0.7594 - val_accuracy: 0.8292\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7586e-04 - accuracy: 1.0000 - val_loss: 0.7992 - val_accuracy: 0.8290\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa310bedaf0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets run the training\n",
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)  # We have a vocab_size of 10000 and embedding dimension of 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we will write the vectors and metadata onto a file\n",
    "\n",
    "We will then load the tsv files and view them on\n",
    "https://projector.tensorflow.org"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}