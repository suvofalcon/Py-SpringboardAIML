{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tensorflow-gpu': conda)"
  },
  "interpreter": {
   "hash": "658eb51e50cabe43eb015cdee70e2f5d0c7abd5e29923e3e04ff83a044688658"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning Assignment - 01 , Problem - 01, Group - 029\n",
    "# Vision Dataset - Animal Image Classification \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Library Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.image import imread\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "## Validate GPU Availability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this code is only valid if GPU is being used for training\n",
    "\n",
    "# Before we run the parameter tuning, we will work with little gpu memory allocation\n",
    "# we will only use that much of memory of gpu as it is needed - allow the growth of gpu memory as it is needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No GPU Available, switching to CPU Version\")"
   ]
  },
  {
   "source": [
    "## Load the dataset and validate the data load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "dataset = train_datagen.flow_from_directory(\"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\", target_size=(32, 32), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir(\"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\"\n",
    "# folder1 = \"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\"\n",
    "\n",
    "imageList=[]\n",
    "classList=[]\n",
    "\n",
    "for file1 in listdir(folder):\n",
    "    file2 = folder + \"/\" + file1\n",
    "    for file3 in listdir(file2):\n",
    "        file4 = file2 + \"/\" + file3\n",
    "        image = tf.keras.preprocessing.image.load_img(file4, target_size=(256, 256))\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        imageList.append(image)\n",
    "        classList.append(file1)\n",
    "\n",
    "# Check whether all the images has been parsed\n",
    "print(f\"Length of the image list - {len(imageList)}\")\n",
    "print(f\"Length of the class list - {len(classList)}\")"
   ]
  },
  {
   "source": [
    "#### Display four images from each of the classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "source": [
    "## Data Preprocessing and Preparation for Neural Network Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert this list into numpy array\n",
    "imageArray = np.asarray(imageList)\n",
    "classArray = np.asarray(classList)\n",
    "\n",
    "print(f\"Shape of the image numpy array - {imageArray.shape}\")\n",
    "print(f\"Shape of the class numpy array - {classArray.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of the imageArray and classArray should now be an ndarray from numpy\n",
    "type(imageArray)"
   ]
  },
  {
   "source": [
    "#### Normalizing our image array\n",
    "we will want all our pixel values to be between 0 and 1 (normalized), in order for the neural net to train faster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageArray = imageArray/255.0"
   ]
  },
  {
   "source": [
    "#### Reshaping the image\n",
    "\n",
    "We are flattening every image meaning we're going to transform each of the dimensions of the image for all the images of the array by flattening all the pixels into a single one vector and we will do that through reshape\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageArray = imageArray.reshape(-1, imageArray.shape[1]*imageArray.shape[2]*imageArray.shape[3])\n",
    "imageArray.shape"
   ]
  },
  {
   "source": [
    "#### Label Encoding the class array"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for \"cats\", \"dogs\", \"pandas\"\n",
    "encoder = LabelEncoder()\n",
    "classArray = encoder.fit_transform(classArray)\n",
    "\n",
    "# Convert to categorical\n",
    "classArray = tf.keras.utils.to_categorical(classArray)\n",
    "classArray.shape"
   ]
  },
  {
   "source": [
    "## Train-Test Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split\n",
    "features_train, features_test, target_train, target_test = train_test_split(imageArray, classArray, test_size=0.3, \n",
    "                                                                            random_state=101)\n",
    "\n",
    "# We will further split the training set into validatoion to evaluate the Neural Network training\n",
    "features_train, features_val, target_train, target_val = train_test_split(features_train, target_train, test_size=0.3, \n",
    "                                                                            random_state=101)\n",
    "\n",
    "print(\"Training Features shape: \", features_train.shape)\n",
    "print(\"Training Target shape: \", target_train.shape)\n",
    "\n",
    "print(\"Validation Features shape: \", features_val.shape)\n",
    "print(\"Validation Target shape: \", target_val.shape)\n",
    "\n",
    "print(\"Test Features shape: \", features_test.shape)\n",
    "print(\"Training Target shape: \", target_test.shape)"
   ]
  },
  {
   "source": [
    "## Building the ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Global Model Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some global Model Constants\n",
    "\n",
    "INPUT_SHAPE = (imageArray.shape[1], )\n",
    "OUTPUT_UNITS = 3\n",
    "HIDDEN_UNITS= 128\n",
    "ACTIVATION_HIDDEN = tf.keras.activations.relu\n",
    "ACTIVATION_OUTPUT = tf.keras.activations.softmax\n",
    "LEARNING_RATE = 1e-3\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "LOSS_FUNCTION = tf.keras.losses.categorical_crossentropy\n",
    "L2_REGULARIZER = tf.keras.regularizers.L2(0.001)\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Metrics - These are the metrics we will evaluate during training\n",
    "\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "          tf.keras.metrics.FalsePositives(name='fp'),\n",
    "          tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "          tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          tf.keras.metrics.Precision(name='precision'),\n",
    "          tf.keras.metrics.Recall(name='recall'),\n",
    "          tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which will build and compile the model\n",
    "\n",
    "'''\n",
    "This will build and compile a model with one hidden layer and 16 neurons\n",
    "'''\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, \n",
    "                                input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN))\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN))\n",
    "    model.add(tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT))\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will build a deep neural network model with multiple hidden layers and implement \n",
    "L2 regularization\n",
    "Dropout regularization\n",
    "'''\n",
    "\n",
    "def make_DNNModel(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, kernel_regularizer=L2_REGULARIZER))\n",
    "    model.add(tf.keras.layers.Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, kernel_regularizer=L2_REGULARIZER))\n",
    "    model.add(tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT))\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot training loss vs validation loss\n",
    "\n",
    "'''\n",
    "This function will take a epoch model from training a neural network\n",
    "Will plot training loss vs validation loss\n",
    "'''\n",
    "\n",
    "def plotTrainLossVsValLoss(epochs_history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    loss_train = epochs_history.history['loss']\n",
    "    loss_val = epochs_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    loss_train = epochs_history.history['loss']\n",
    "    loss_val = epochs_history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, (EPOCHS + 1))\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "    plt.title('Training Loss vs Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot training accuracy vs validation accuracy\n",
    "\n",
    "'''\n",
    "This function will take a epoch model from training a neural network\n",
    "Will plot training accuracy vs validation accuracy\n",
    "'''\n",
    "\n",
    "def plotTrainAccuracyVsValAccuracy(epochs_history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    loss_train = epochs_history.history['accuracy']\n",
    "    loss_val = epochs_history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(1, (EPOCHS + 1))\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training Accuracy')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training Accuracy vs Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "#### Compile the Model and Check the summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build the model and see the mmodel summary\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# we will now train the model on training and validation data\n",
    "# Now use the function to plot the confusion matrix\n",
    "\n",
    "start = datetime.now()\n",
    "epochs_history_simple = model.fit(features_train, target_train, epochs=EPOCHS,\n",
    "                          validation_data=(features_val, target_val),\n",
    "                          verbose=1)\n",
    "end = datetime.now()\n",
    "print(f\"The training of simple model completed in time - {end - start}\")"
   ]
  },
  {
   "source": [
    "#### Check Performance Graphs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs validation loss\n",
    "plotTrainLossVsValLoss(epochs_history=epochs_history_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training accuracy vs Validation accuracy\n",
    "plotTrainAccuracyVsValAccuracy(epochs_history=epochs_history_simple)"
   ]
  },
  {
   "source": [
    "## Model Prediction\n",
    "\n",
    "#### We will run the predictions and display the confusion matrix and classification report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use it in Confusion Matrix because its a multiclass prediction\n",
    "target_predictions = np.argmax(model.predict(features_test), axis = 1)\n",
    "target_test = np.argmax(target_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"\\n\")\n",
    "print(confusion_matrix(target_test, target_predictions))\n",
    "print(\"\\n\")\n",
    "print('Classification Report')\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, target_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}