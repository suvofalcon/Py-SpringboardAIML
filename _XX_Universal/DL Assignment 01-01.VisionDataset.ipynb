{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tensorflow-gpu': conda)"
  },
  "interpreter": {
   "hash": "658eb51e50cabe43eb015cdee70e2f5d0c7abd5e29923e3e04ff83a044688658"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning Assignment - 01 , Problem - 01, Group - 029\n",
    "# Vision Dataset - Animal Image Classification \n",
    "\n",
    "https://www.kaggle.com/rwt1998/animal-classification\n",
    "\n",
    "https://www.kaggle.com/bygbrains/dog-cat-pandas-image-classifier\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Library Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "## Validate GPU Availability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Note this code is only valid if GPU is being used for training\n",
    "\n",
    "# Before we run the parameter tuning, we will work with little gpu memory allocation\n",
    "# we will only use that much of memory of gpu as it is needed - allow the growth of gpu memory as it is needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No GPU Available, switching to CPU Version\")"
   ]
  },
  {
   "source": [
    "## Load the dataset and validate the data load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 3000 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "dataset = train_datagen.flow_from_directory(\"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\", target_size=(32, 32), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cats', 'dogs', 'panda']"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "listdir(\"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of the image list - 3000\nLength of the class list - 3000\n"
     ]
    }
   ],
   "source": [
    "folder = \"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\"\n",
    "# folder1 = \"/home/suvo/Documents/LargeDatasets/CNNDatasets/Cats-Dogs-Pandas/animals\"\n",
    "\n",
    "imageList=[]\n",
    "classList=[]\n",
    "\n",
    "for file1 in listdir(folder):\n",
    "    file2 = folder + \"/\" + file1\n",
    "    for file3 in listdir(file2):\n",
    "        file4 = file2 + \"/\" + file3\n",
    "        image = tf.keras.preprocessing.image.load_img(file4, target_size=(128, 128))\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        imageList.append(image)\n",
    "        classList.append(file1)\n",
    "\n",
    "# Check whether all the images has been parsed\n",
    "print(f\"Length of the image list - {len(imageList)}\")\n",
    "print(f\"Length of the class list - {len(classList)}\")"
   ]
  },
  {
   "source": [
    "#### Display four images from each of the classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the image numpy array - (3000, 128, 128, 3)\nShape of the class numpy array - (3000,)\n"
     ]
    }
   ],
   "source": [
    "# We will convert this list into numpy array\n",
    "imageArray = np.asarray(imageList)\n",
    "classArray = np.asarray(classList)\n",
    "\n",
    "print(f\"Shape of the image numpy array - {imageArray.shape}\")\n",
    "print(f\"Shape of the class numpy array - {classArray.shape}\")"
   ]
  },
  {
   "source": [
    "#### Normalizing the images\n",
    "\n",
    "we will want all our pixel values to be between 0 and 1 (normalized), in order for the neural net to train faster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageArray = imageArray/255.0"
   ]
  },
  {
   "source": [
    "#### Reshaping the image\n",
    "\n",
    "We are flattening every image meaning we're going to transform each of the dimensions of the image for all the images of the array by flattening all the pixels into a single one vector and we will do that through reshape\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3000, 49152)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "imageArray = imageArray.reshape(-1, imageArray.shape[1]*imageArray.shape[2]*imageArray.shape[3])\n",
    "imageArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['cats', 'cats', 'cats', ..., 'panda', 'panda', 'panda'],\n",
       "      dtype='<U5')"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "classArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.09803922, 0.11372549, 0.14901961, ..., 0.62352943, 0.5882353 ,\n",
       "       0.5686275 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "imageArray[0]"
   ]
  },
  {
   "source": [
    "## Train-Test Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features shape:  (1470, 49152)\nTraining Target shape:  (1470,)\nValidation Features shape:  (630, 49152)\nValidation Target shape:  (630,)\nTest Features shape:  (900, 49152)\nTraining Target shape:  (900,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the split\n",
    "features_train, features_test, target_train, target_test = train_test_split(imageArray, classArray, test_size=0.3, \n",
    "                                                                            random_state=101)\n",
    "\n",
    "# We will further split the training set into validatoion to evaluate the Neural Network training\n",
    "features_train, features_val, target_train, target_val = train_test_split(features_train, target_train, test_size=0.3, \n",
    "                                                                            random_state=101)\n",
    "\n",
    "print(\"Training Features shape: \", features_train.shape)\n",
    "print(\"Training Target shape: \", target_train.shape)\n",
    "\n",
    "print(\"Validation Features shape: \", features_val.shape)\n",
    "print(\"Validation Target shape: \", target_val.shape)\n",
    "\n",
    "print(\"Test Features shape: \", features_test.shape)\n",
    "print(\"Training Target shape: \", target_test.shape)"
   ]
  },
  {
   "source": [
    "## Building the ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Global Model Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "imageArray.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some global Model Constants\n",
    "\n",
    "INPUT_SHAPE = (imageArray.shape[1], )\n",
    "OUTPUT_UNITS = 3\n",
    "HIDDEN_UNITS= 128\n",
    "ACTIVATION_HIDDEN = tf.keras.activations.relu\n",
    "ACTIVATION_OUTPUT = tf.keras.activations.softmax\n",
    "LEARNING_RATE = 1e-3\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "LOSS_FUNCTION = tf.keras.losses.sparse_categorical_crossentropy\n",
    "L2_REGULARIZER = tf.keras.regularizers.L2(0.001)\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Metrics - These are the metrics we will evaluate during training\n",
    "\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "          tf.keras.metrics.FalsePositives(name='fp'),\n",
    "          tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "          tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          tf.keras.metrics.Precision(name='precision'),\n",
    "          tf.keras.metrics.Recall(name='recall'),\n",
    "          tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which will build and compile the model\n",
    "\n",
    "'''\n",
    "This will build and compile a model with one hidden layer and 16 neurons\n",
    "'''\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, \n",
    "                                input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN))\n",
    "    model.add(tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT))\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will build a deep neural network model with multiple hidden layers and implement \n",
    "L2 regularization\n",
    "Dropout regularization\n",
    "'''\n",
    "\n",
    "def make_DNNModel(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, kernel_regularizer=L2_REGULARIZER))\n",
    "    model.add(tf.keras.layers.Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN, kernel_regularizer=L2_REGULARIZER))\n",
    "    model.add(tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT))\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot training loss vs validation loss\n",
    "\n",
    "'''\n",
    "This function will take a epoch model from training a neural network\n",
    "Will plot training loss vs validation loss\n",
    "'''\n",
    "\n",
    "def plotTrainLossVsValLoss(epochs_history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    loss_train = epochs_history.history['loss']\n",
    "    loss_val = epochs_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    loss_train = epochs_history.history['loss']\n",
    "    loss_val = epochs_history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, (EPOCHS + 1))\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "    plt.title('Training Loss vs Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot training accuracy vs validation accuracy\n",
    "\n",
    "'''\n",
    "This function will take a epoch model from training a neural network\n",
    "Will plot training accuracy vs validation accuracy\n",
    "'''\n",
    "\n",
    "def plotTrainAccuracyVsValAccuracy(epochs_history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    loss_train = epochs_history.history['accuracy']\n",
    "    loss_val = epochs_history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(1, (EPOCHS + 1))\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training Accuracy')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training Accuracy vs Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot the confusion matrix\n",
    "# Let us visualize the Confusion Matrix and detail out some key metrices including classification report\n",
    "\n",
    "'''\n",
    "This function will plot the confusion matrix \n",
    "This will also display various performance metrices\n",
    "'''\n",
    "def plot_cm(labels, predictions, threshold=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > threshold)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix %0.2f\" %threshold)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    \n",
    "    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "    print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "    print(\"\\n\")\n",
    "    print(\"F1-Score\")\n",
    "    print(f1_score(target_test, target_predictions > 0.5))\n",
    "    print(\"\\n\")\n",
    "    print(\"Accuracy Score\")\n",
    "    print(accuracy_score(target_test, target_predictions > threshold))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(target_test, target_predictions > 0.5))"
   ]
  },
  {
   "source": [
    "#### Compile the Model and Check the summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_7 (Dense)              (None, 128)               6291584   \n_________________________________________________________________\ndense_8 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_9 (Dense)              (None, 3)                 387       \n=================================================================\nTotal params: 6,308,483\nTrainable params: 6,308,483\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Lets build the model and see the mmodel summary\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-89ed0bbeeb11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m epochs_history_simple = model.fit(features_train, target_train, epochs=EPOCHS,\n\u001b[0m\u001b[1;32m      6\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                           verbose=1)\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tensorflow-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tensorflow-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tensorflow-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# we will now train the model on training and validation data\n",
    "# Now use the function to plot the confusion matrix\n",
    "\n",
    "start = datetime.now()\n",
    "epochs_history_simple = model.fit(features_train, target_train, epochs=EPOCHS,\n",
    "                          validation_data=(features_val, target_val),\n",
    "                          verbose=1)\n",
    "end = datetime.now()\n",
    "print(f\"The training of simple model completed in time - {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}