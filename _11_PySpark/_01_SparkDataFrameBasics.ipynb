{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Basics\n",
    "\n",
    "Comprehensive code to demonstrate various basics of a Spark DataFrame\n",
    "\n",
    "park DataFrames are the workhouse and main way of working with Spark and Python post Spark 2.0. DataFrames act as powerful versions of tables, with rows and columns, easily handling large datasets. The shift to DataFrames provides many advantages:\n",
    "\n",
    "A much simpler syntax\n",
    "\n",
    "Ability to use SQL directly in the dataframe\n",
    "Operations are automatically distributed across RDDs\n",
    "\n",
    "If you've used R or even the pandas library with Python you are probably already familiar with the concept of DataFrames. Spark DataFrame expand on a lot of these concepts, allowing you to transfer that knowledge easily by understanding the simple syntax of Spark DataFrames. Remember that the main advantage to using Spark DataFrames vs those other programs is that Spark can handle data across many RDDs, huge data sets that would never fit on a single computer. That comes at a slight cost of some \"peculiar\" syntax choices, but after this course you will feel very comfortable with all those topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the spark session <br>\n",
    "Initialize the Spark Session and assigh application Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data. This time the dataset is a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('resources/people.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the schema (info of the data structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the statistical summary of the columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------------------+-------+\n|summary|               age|   name|\n+-------+------------------+-------+\n|  count|                 2|      3|\n|   mean|              24.5|   null|\n| stddev|7.7781745930520225|   null|\n|    min|                19|   Andy|\n|    max|                30|Michael|\n+-------+------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can specify the schema while we read the data from other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the list of Structure fields\n",
    "\n",
    "* :param name: string, name of the field.\n",
    "* :param dataType: :class:`DataType` of the field.\n",
    "* :param nullable: boolean, whether the field can be null (None) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True), StructField(\"name\", StringType(), True)]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)\n",
    "final_struc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Now while reading the data , we will specify this schema\n",
    "\n",
    "df = spark.read.json('resources/people.json', schema=final_struc)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing Data from a Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check the type of the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Order to select a particular column and see it data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df.select('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return all the values of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n| age|\n+----+\n|null|\n|  30|\n|  19|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return first two rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Select multiple columns and return its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new column with a simple copy from existing one. This returns a new data frame with the newly added column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+------+\n| age|   name|newage|\n+----+-------+------+\n|null|Michael|  null|\n|  30|   Andy|    30|\n|  19| Justin|    19|\n+----+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('newage', df['age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform some operation while this column gets added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+----------+\n| age|   name|double_age|\n+----+-------+----------+\n|null|Michael|      null|\n|  30|   Andy|        60|\n|  19| Justin|        38|\n+----+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+\n|new_age|   name|\n+-------+-------+\n|   null|Michael|\n|     30|   Andy|\n|     19| Justin|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age', 'new_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQLs on Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SQL Queries directly, we need to register it to a temporary view. View is basically a pointer to the spark\n",
    "data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT * FROM people\")\n",
    "\n",
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people WHERE age=30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations on Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('resources/appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Date: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n |-- Adj Close: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Now we will check the schema\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date='2010-01-05', Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002),\n",
       " Row(Date='2010-01-06', Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004)]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# See the first few rows\n",
    "df.head(3)  # This returns a list object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Also the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n+----------+----------+----------+------------------+----------+---------+------------------+\n|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n+----------+----------+----------+------------------+----------+---------+------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Also show specific number of rows\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "A large part of working with DataFrames is the ability to quickly filter out data based on conditions. Spark DataFrames are built on top of the Spark SQL platform, which means that is you already know SQL, you can quickly and easily grab that data using SQL commands, or using the DataFrame methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# We will do that using SQL first \n",
    "\n",
    "# All entries where Closing value is less than 500\n",
    "df.filter(\"Close<500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the filtered set, if we need to see only a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+\n|      Date|              Open|\n+----------+------------------+\n|2010-01-04|        213.429998|\n|2010-01-05|        214.599998|\n|2010-01-06|        214.379993|\n|2010-01-07|            211.75|\n|2010-01-08|        210.299994|\n|2010-01-11|212.79999700000002|\n|2010-01-12|209.18999499999998|\n|2010-01-13|        207.870005|\n|2010-01-14|210.11000299999998|\n|2010-01-15|210.92999500000002|\n|2010-01-19|        208.330002|\n|2010-01-20|        214.910006|\n|2010-01-21|        212.079994|\n|2010-01-22|206.78000600000001|\n|2010-01-25|202.51000200000001|\n|2010-01-26|205.95000100000001|\n|2010-01-27|        206.849995|\n|2010-01-28|        204.930004|\n|2010-01-29|        201.079996|\n|2010-02-01|192.36999699999998|\n+----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close<500\").select(['Date', 'Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+------------------+\n|              Open|             Close|\n+------------------+------------------+\n|        213.429998|        214.009998|\n|        214.599998|        214.379993|\n|        214.379993|        210.969995|\n|            211.75|            210.58|\n|        210.299994|211.98000499999998|\n|212.79999700000002|210.11000299999998|\n|209.18999499999998|        207.720001|\n|        207.870005|        210.650002|\n|210.11000299999998|            209.43|\n|210.92999500000002|            205.93|\n|        208.330002|        215.039995|\n|        214.910006|            211.73|\n|        212.079994|        208.069996|\n|206.78000600000001|            197.75|\n|202.51000200000001|        203.070002|\n|205.95000100000001|        205.940001|\n|        206.849995|        207.880005|\n|        204.930004|        199.289995|\n|        201.079996|        192.060003|\n|192.36999699999998|        194.729998|\n+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# We can extract multiple columns using SQL with select()\n",
    "\n",
    "df.filter(\"Close < 500\").select(['Open', 'Close']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the normal python operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n|2010-02-02|        195.909998|        196.319994|193.37999299999998|        195.859997|174585600|25.375532999999997|\n|2010-02-03|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n|2010-02-04|        196.730003|        198.370001|        191.570005|        192.050003|189413000|         24.881912|\n|2010-02-05|192.63000300000002|             196.0|        190.850002|        195.460001|212576700|25.323710000000002|\n|2010-02-08|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n|2010-02-09|        196.419996|        197.499994|        194.749998|196.19000400000002|158221700|         25.418289|\n|2010-02-10|        195.889997|             196.6|            194.26|195.12000700000002| 92590400|          25.27966|\n|2010-02-11|        194.880001|        199.750006|194.05999599999998|        198.669994|137586400|         25.739595|\n|2010-02-23|        199.999998|        201.330002|        195.709993|        197.059998|143773700|         25.531005|\n|2014-06-09|         92.699997|         93.879997|             91.75|         93.699997| 75415000|         88.906324|\n|2014-06-10|         94.730003|         95.050003|             93.57|             94.25| 62777000|         89.428189|\n|2014-06-11|         94.129997|         94.760002|         93.470001|         93.860001| 45681000|         89.058142|\n|2014-06-12|         94.040001|         94.120003|         91.900002|         92.290001| 54749000|         87.568463|\n|2014-06-13|         92.199997|         92.440002|         90.879997|         91.279999| 54525000|         86.610132|\n|2014-06-16|         91.510002|             92.75|         91.449997|         92.199997| 35561000|         87.483064|\n|2014-06-17|         92.309998|         92.699997|         91.800003| 92.08000200000001| 29726000| 87.36920699999999|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 200).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use logical operators for selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+----------+----------+----------+---------+------------------+\n|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n+----------+------------------+----------+----------+----------+---------+------------------+\n|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n+----------+------------------+----------+----------+----------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 200) & (df['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 200) | (df['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using and with not operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+----------+----------+----------+---------+------------------+\n|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n+----------+------------------+----------+----------+----------+---------+------------------+\n|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n+----------+------------------+----------+----------+----------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 200) & ~(df['Open'] < 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+----------+------+------+---------+---------+\n|      Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n+----------+------------------+----------+------+------+---------+---------+\n|2010-01-22|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n+----------+------------------+----------+------+------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Low'] == 197.16).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will collect these results as python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(Date='2010-01-22', Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "df.filter(df['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "result = df.filter(df['Low'] == 197.16).collect()\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns a list of row objects. In order to see the type of the first object in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can be converted into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Date': '2010-01-22',\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "row = result[0]\n",
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print individual items (column values) from the row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2010-01-22\n206.78000600000001\n207.499996\n197.16\n197.75\n220441900\n25.620401\n"
     ]
    }
   ],
   "source": [
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Date - 2010-01-22\nOpen - 206.78000600000001\nHigh - 207.499996\nLow - 197.16\nClose - 197.75\nVolume - 220441900\nAdj Close - 25.620401\n"
     ]
    }
   ],
   "source": [
    "# We can also print the dictionary along with keys\n",
    "rowDict = row.asDict()\n",
    "\n",
    "for key, value in rowDict.items():\n",
    "    print(key, \"-\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate Functions\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Company: string (nullable = true)\n |-- Person: string (nullable = true)\n |-- Sales: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Now we will read another data\n",
    "\n",
    "df = spark.read.csv('resources/sales_info.csv', inferSchema=True, header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-----+\n|Company| Person|Sales|\n+-------+-------+-----+\n|   GOOG|    Sam|200.0|\n|   GOOG|Charlie|120.0|\n|   GOOG|  Frank|340.0|\n|   MSFT|   Tina|600.0|\n|   MSFT|    Amy|124.0|\n|   MSFT|Vanessa|243.0|\n|     FB|   Carl|870.0|\n|     FB|  Sarah|350.0|\n|   APPL|   John|250.0|\n|   APPL|  Linda|130.0|\n|   APPL|   Mike|750.0|\n|   APPL|  Chris|350.0|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----------------+\n|Company|       avg(Sales)|\n+-------+-----------------+\n|   APPL|            370.0|\n|   GOOG|            220.0|\n|     FB|            610.0|\n|   MSFT|322.3333333333333|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# We will group together by company.. this will return a grouped data off which various methods can be called\n",
    "\n",
    "df.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+\n|Company|count|\n+-------+-----+\n|   APPL|    4|\n|   GOOG|    3|\n|     FB|    2|\n|   MSFT|    3|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# We can get count of values\n",
    "df.groupBy('Company').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n|Company|sum(Sales)|\n+-------+----------+\n|   APPL|    1480.0|\n|   GOOG|     660.0|\n|     FB|    1220.0|\n|   MSFT|     967.0|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# groupby company, sum of sales\n",
    "df.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max, Min and Sum\n",
    "df.groupBy('Company').max().show()\n",
    "df.groupBy('Company').min().show()\n",
    "df.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this link for more info on other methods:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module\n",
    "\n",
    "Not all methods need a groupby call, instead you can just call the generalized .agg() method, that will call the aggregate across all rows in the dataframe column specified. It can take in arguments as a single column, or create multiple aggregate calls all at once using dictionary notation.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n|max(Sales)|\n+----------+\n|     870.0|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Max sales across everything\n",
    "df.agg({'Sales': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n|Company|max(Sales)|\n+-------+----------+\n|   APPL|     750.0|\n|   GOOG|     340.0|\n|     FB|     870.0|\n|   MSFT|     600.0|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# We could have done this on the groupby object as well\n",
    "\n",
    "df.groupBy('Company').agg({'Sales': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n|Company|sum(Sales)|\n+-------+----------+\n|   APPL|    1480.0|\n|   GOOG|     660.0|\n|     FB|    1220.0|\n|   MSFT|     967.0|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Or aggregate sales total by company\n",
    "df.groupBy('Company').agg({'Sales': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "There are a variety of functions you can import from pyspark.sql.functions. Check out the documentation for the full list available:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import countDistinct, avg, stddev, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n|count(DISTINCT Sales)|\n+---------------------+\n|                   11|\n+---------------------+\n\n+-----------------------+\n|count(DISTINCT Company)|\n+-----------------------+\n|                      4|\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"Sales\")).show()\n",
    "df.select(countDistinct(\"Company\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+\n|Distinct Sales|\n+--------------+\n|            11|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# If we want to change the name, we can use alias\n",
    "\n",
    "df.select(countDistinct(\"Sales\").alias('Distinct Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+\n|       avg(Sales)|\n+-----------------+\n|360.5833333333333|\n+-----------------+\n\n+------------------+\n|               std|\n+------------------+\n|250.08742410799007|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales')).show()\n",
    "df.select(stddev('Sales').alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+\n|Distinct Companies|\n+------------------+\n|                 4|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# similarly distinch companies\n",
    "df.select(countDistinct(\"Company\").alias('Distinct Companies')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n|format_number(std, 2)|\n+---------------------+\n|               250.09|\n+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# In order to specify a precision digits\n",
    "\n",
    "df.select(stddev('Sales').alias('std')).select(format_number('std', 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-----+\n|Company| Person|Sales|\n+-------+-------+-----+\n|   GOOG|Charlie|120.0|\n|   MSFT|    Amy|124.0|\n|   APPL|  Linda|130.0|\n|   GOOG|    Sam|200.0|\n|   MSFT|Vanessa|243.0|\n|   APPL|   John|250.0|\n|   GOOG|  Frank|340.0|\n|     FB|  Sarah|350.0|\n|   APPL|  Chris|350.0|\n|   MSFT|   Tina|600.0|\n|   APPL|   Mike|750.0|\n|     FB|   Carl|870.0|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Orderby Ascending\n",
    "\n",
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-----+\n|Company| Person|Sales|\n+-------+-------+-----+\n|     FB|   Carl|870.0|\n|   APPL|   Mike|750.0|\n|   MSFT|   Tina|600.0|\n|     FB|  Sarah|350.0|\n|   APPL|  Chris|350.0|\n|   GOOG|  Frank|340.0|\n|   APPL|   John|250.0|\n|   MSFT|Vanessa|243.0|\n|   GOOG|    Sam|200.0|\n|   APPL|  Linda|130.0|\n|   MSFT|    Amy|124.0|\n|   GOOG|Charlie|120.0|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Descending call off the column itself.\n",
    "df.orderBy(df[\"Sales\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n|Company|sum(Sales)|\n+-------+----------+\n|   APPL|    1480.0|\n|     FB|    1220.0|\n|   MSFT|     967.0|\n|   GOOG|     660.0|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Combining Orderby with Aggregate\n",
    "\n",
    "df.groupBy('Company').agg({'Sales': 'sum'}).sort(func.col('sum(Sales)').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data\n",
    "\n",
    "Often data sources are incomplete, which means you will have missing data, you have 3 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "* Just keep the missing data points.\n",
    "* Drop them missing data points (including the entire row)\n",
    "* Fill them in with some other value.\n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data frame\n",
    "\n",
    "df = spark.read.csv(\"resources/ContainsNull.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data remains as null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.\n",
    "    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "    * param thresh: int, default None\n",
    "    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# drop any row that contains missing data\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# has to have atleast 2 NON-null values\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# If we have to drop missing values from a specific column\n",
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have to drop missing value if any value of a column in a row as a missing value\n",
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have to drop missing value only if all value of a column in a row as a missing value\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('NEW VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('No Name',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the mean value for the column, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "\n",
    "# Weird nested formatting of Row object!\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|400.5|\n|emp2| null|400.5|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_val[0][0], ['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Dates and Timestamps\n",
    "\n",
    "You will often find yourself working with Time and Date information, let's walk through some ways you can deal with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "\n",
    "df = spark.read.csv(\"resources/appl_stock.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets walkthrough to grab the parts of the timestamp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number, dayofmonth, hour, dayofyear, month, year, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+\n|dayofmonth(Date)|\n+----------------+\n|               4|\n|               5|\n|               6|\n|               7|\n|               8|\n|              11|\n|              12|\n|              13|\n|              14|\n|              15|\n|              19|\n|              20|\n|              21|\n|              22|\n|              25|\n|              26|\n|              27|\n|              28|\n|              29|\n|               1|\n+----------------+\nonly showing top 20 rows\n\nShowing hour\n+----------+\n|hour(Date)|\n+----------+\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n|         0|\n+----------+\nonly showing top 20 rows\n\nShowing day of year\n+---------------+\n|dayofyear(Date)|\n+---------------+\n|              4|\n|              5|\n|              6|\n|              7|\n|              8|\n|             11|\n|             12|\n|             13|\n|             14|\n|             15|\n|             19|\n|             20|\n|             21|\n|             22|\n|             25|\n|             26|\n|             27|\n|             28|\n|             29|\n|             32|\n+---------------+\nonly showing top 20 rows\n\nShowing month\n+-----------+\n|month(Date)|\n+-----------+\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          1|\n|          2|\n+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofmonth(df['Date'])).show()\n",
    "print(f\"Showing hour\")\n",
    "df.select(hour(df['Date'])).show()\n",
    "print(f\"Showing day of year\")\n",
    "df.select(dayofyear(df['Date'])).show()\n",
    "print(f\"Showing month\")\n",
    "df.select(month(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, let's say we wanted to know the average closing price per year. Easy! With a groupby and the year() function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.withColumn(\"Year\", year(df['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+------------------+\n|Year|        avg(Close)|\n+----+------------------+\n|2015|120.03999980555547|\n|2013| 472.6348802857143|\n|2014| 295.4023416507935|\n|2012| 576.0497195640002|\n|2016|104.60400786904763|\n|2010| 259.8424600000002|\n|2011|364.00432532142867|\n+----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf.groupBy('Year').agg({'Close': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pyspark': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "a8606bb89ff58952fb9d896d3ff97deb9accae9cf8d913f17f7f7ec9d60aa4a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}