{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sarcasm Detection using Tensorflow - Bidirectional LSTMs and Convolutional 1D\n",
    "\n",
    "Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\n",
    "\n",
    "To overcome the limitations related to noise in Twitter datasets, this News Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.\n",
    "\n",
    "This new dataset has following advantages over the existing Twitter datasets:\n",
    "\n",
    "Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n",
    "\n",
    "Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n",
    "\n",
    "Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.\n",
    "\n",
    "**Content**\n",
    "Each record consists of three attributes:\n",
    "\n",
    "is_sarcastic: 1 if the record is sarcastic otherwise 0\n",
    "\n",
    "headline: the headline of the news article\n",
    "\n",
    "article_link: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "***\n",
    "The concept is to perform tokenizing the words to get numeric values from them and then using Embeddings to group words of similar meaning,\n",
    "depending on how they are labelled resulting in a good but rough sentiment analysis\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the json data\n",
    "\n",
    "def parse_data(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)\n",
    "\n",
    "datastore = list(parse_data('resources/Sarcasm_Headlines_Dataset.json'))\n",
    "\n",
    "# define the attribues from the json data\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define some global constants\n",
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<oov>\"\n",
    "training_size = 20000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Build the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           16000     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 30,129\n",
      "Trainable params: 30,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model and compile\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:46:37.373557: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "625/625 [==============================] - 21s 29ms/step - loss: 0.5521 - accuracy: 0.6824 - val_loss: 0.3861 - val_accuracy: 0.8223\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.3569 - accuracy: 0.8337 - val_loss: 0.3776 - val_accuracy: 0.8241\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3357 - accuracy: 0.8464 - val_loss: 0.3712 - val_accuracy: 0.8299\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.3156 - accuracy: 0.8624 - val_loss: 0.3710 - val_accuracy: 0.8301\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.3056 - accuracy: 0.8642 - val_loss: 0.3763 - val_accuracy: 0.8326\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2965 - accuracy: 0.8670 - val_loss: 0.3773 - val_accuracy: 0.8346\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 15s 25ms/step - loss: 0.2806 - accuracy: 0.8783 - val_loss: 0.3770 - val_accuracy: 0.8317\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2712 - accuracy: 0.8830 - val_loss: 0.3805 - val_accuracy: 0.8362\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - 15s 25ms/step - loss: 0.2688 - accuracy: 0.8816 - val_loss: 0.3859 - val_accuracy: 0.8278\n",
      "Epoch 10/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2608 - accuracy: 0.8880 - val_loss: 0.4143 - val_accuracy: 0.8232\n",
      "Epoch 11/50\n",
      "625/625 [==============================] - 15s 25ms/step - loss: 0.2507 - accuracy: 0.8935 - val_loss: 0.3982 - val_accuracy: 0.8308\n",
      "Epoch 12/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2380 - accuracy: 0.8969 - val_loss: 0.3947 - val_accuracy: 0.8305\n",
      "Epoch 13/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2430 - accuracy: 0.8975 - val_loss: 0.4202 - val_accuracy: 0.8228\n",
      "Epoch 14/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2292 - accuracy: 0.9038 - val_loss: 0.4246 - val_accuracy: 0.8228\n",
      "Epoch 15/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2269 - accuracy: 0.9043 - val_loss: 0.4339 - val_accuracy: 0.8271\n",
      "Epoch 16/50\n",
      "625/625 [==============================] - 16s 26ms/step - loss: 0.2226 - accuracy: 0.9065 - val_loss: 0.4436 - val_accuracy: 0.8237\n",
      "Epoch 17/50\n",
      "625/625 [==============================] - 16s 26ms/step - loss: 0.2151 - accuracy: 0.9102 - val_loss: 0.4575 - val_accuracy: 0.8199\n",
      "Epoch 18/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.2100 - accuracy: 0.9125 - val_loss: 0.4607 - val_accuracy: 0.8177\n",
      "Epoch 19/50\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.1921 - accuracy: 0.9207 - val_loss: 0.4995 - val_accuracy: 0.8143\n",
      "Epoch 20/50\n",
      "625/625 [==============================] - 855s 1s/step - loss: 0.1873 - accuracy: 0.9254 - val_loss: 0.5633 - val_accuracy: 0.8098\n",
      "Epoch 21/50\n",
      " 27/625 [>.............................] - ETA: 14s - loss: 0.2243 - accuracy: 0.8971"
     ]
    }
   ],
   "source": [
    "# fit on the training data\n",
    "num_epochs = 50\n",
    "training_padded = np.array(training_padded)\n",
    "testing_padded = np.array(testing_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                    validation_data=(testing_padded, testing_labels), verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}