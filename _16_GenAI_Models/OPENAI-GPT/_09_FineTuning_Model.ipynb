{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tuning a Model - ChatBot Example\n",
    "\n",
    "In this project, we'll explore how to fine-tune a GPT model such as text-babbage model with our own data set. You should note, this may not be needed for more advanced text-davinci models or future GPT-4 models, but let's explore the process of creating our \n",
    "own custom fine-tuning data set, formatting it for OpenAI, and then training and calling our own custom model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8226b80dd5d52778"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Library Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e7a7e68414cade9"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:51:44.083311Z",
     "start_time": "2023-10-12T06:51:42.814008Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Q&A Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d121c2cd3a5e2254"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      Id  OwnerUserId          CreationDate            ClosedDate  Score  \\\n0  11060        912.0  2008-08-14T13:59:21Z                   NaN     18   \n1  17250        394.0  2008-08-20T00:16:40Z                   NaN     24   \n2  31340     242853.0  2008-08-27T23:44:47Z                   NaN     71   \n3  34020       3561.0  2008-08-29T05:43:16Z                   NaN     17   \n4  34570        577.0  2008-08-29T16:10:41Z  2011-11-08T16:11:43Z     13   \n\n                                               Title  \\\n0           How should I unit test a code-generator?   \n1             Create an encrypted ZIP file in Python   \n2  How do threads work in Python, and what are co...   \n3                          Are Python threads buggy?   \n4  What is the best quick-read Python book out th...   \n\n                                                Body  ParentId  \\\n0  This is a difficult and open-ended question I ...     11060   \n1  I'm creating an ZIP file with ZipFile in Pytho...     17250   \n2  I've been trying to wrap my head around how th...     31340   \n3  A reliable coder friend told me that Python's ...     34020   \n4  I am taking a class that requires Python. We w...     34570   \n\n                                              Answer  \n0  I started writing up a summary of my experienc...  \n1  I created a simple library to create a passwor...  \n2  Yes, because of the Global Interpreter Lock (G...  \n3  Python threads are good for concurrent I/O pro...  \n4  I loved Dive Into Python, especially if you're...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ClosedDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n      <th>ParentId</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11060</td>\n      <td>912.0</td>\n      <td>2008-08-14T13:59:21Z</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>How should I unit test a code-generator?</td>\n      <td>This is a difficult and open-ended question I ...</td>\n      <td>11060</td>\n      <td>I started writing up a summary of my experienc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17250</td>\n      <td>394.0</td>\n      <td>2008-08-20T00:16:40Z</td>\n      <td>NaN</td>\n      <td>24</td>\n      <td>Create an encrypted ZIP file in Python</td>\n      <td>I'm creating an ZIP file with ZipFile in Pytho...</td>\n      <td>17250</td>\n      <td>I created a simple library to create a passwor...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31340</td>\n      <td>242853.0</td>\n      <td>2008-08-27T23:44:47Z</td>\n      <td>NaN</td>\n      <td>71</td>\n      <td>How do threads work in Python, and what are co...</td>\n      <td>I've been trying to wrap my head around how th...</td>\n      <td>31340</td>\n      <td>Yes, because of the Global Interpreter Lock (G...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34020</td>\n      <td>3561.0</td>\n      <td>2008-08-29T05:43:16Z</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>Are Python threads buggy?</td>\n      <td>A reliable coder friend told me that Python's ...</td>\n      <td>34020</td>\n      <td>Python threads are good for concurrent I/O pro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34570</td>\n      <td>577.0</td>\n      <td>2008-08-29T16:10:41Z</td>\n      <td>2011-11-08T16:11:43Z</td>\n      <td>13</td>\n      <td>What is the best quick-read Python book out th...</td>\n      <td>I am taking a class that requires Python. We w...</td>\n      <td>34570</td>\n      <td>I loved Dive Into Python, especially if you're...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv(\"/Volumes/Data/Datasets/genai_datasets/python_qa.csv\")\n",
    "data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:13.237930Z",
     "start_time": "2023-10-12T06:52:12.253115Z"
    }
   },
   "id": "b151fe293ced5a56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formatting for Fine Tuning\n",
    "\n",
    "The formatting for a fine-tuning data set involves a prompt and expected completion. This leads fine-tuning to be a great choice for dialogue instances, such as question and answer or customer support.\n",
    "\n",
    "The format should look like the following (a list of dictionaries): <br><br>\n",
    "\n",
    "    [{\"prompt\": \"some prompt string\",\"completion\":\"the best completed text option given the prompt\"},]\n",
    "    \n",
    "\n",
    "Convert the information from CSV to the fine tuning format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f39f4e766578e86"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "questions, answers = data_frame[\"Body\"], data_frame[\"Answer\"] # Use tuple Unpacking"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:19.535348Z",
     "start_time": "2023-10-12T06:52:19.530889Z"
    }
   },
   "id": "9aa23c5646778716"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0    This is a difficult and open-ended question I ...\n1    I'm creating an ZIP file with ZipFile in Pytho...\n2    I've been trying to wrap my head around how th...\n3    A reliable coder friend told me that Python's ...\n4    I am taking a class that requires Python. We w...\nName: Body, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:21.298386Z",
     "start_time": "2023-10-12T06:52:21.292442Z"
    }
   },
   "id": "2b6294e43d07a857"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0    I started writing up a summary of my experienc...\n1    I created a simple library to create a passwor...\n2    Yes, because of the Global Interpreter Lock (G...\n3    Python threads are good for concurrent I/O pro...\n4    I loved Dive Into Python, especially if you're...\nName: Answer, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:22.367090Z",
     "start_time": "2023-10-12T06:52:22.362347Z"
    }
   },
   "id": "1dd7b202df1fdd4a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'prompt': \"I am starting to use Python (specifically because of Django) and I would like to remove the burden for exhaustive testing by performing some static analysis.  What tools/parameters/etc. exist to detect issues at compile time that would otherwise show up during runtime? (type errors are probably the most obvious case of this, but undefined variables are another big one that could be avoided with an in-depth analysis of the AST.)\\n\\nObviously testing is important, and I don't imply that tests can be obviated entirely; however, there are many runtime errors in python that are not possible in other languages that perform stricter run-time checking -- I'm hoping that there are tools to bring at least some of these capabilities to python as well.\\n\",\n 'completion': \"pylint is the best such tool I've found. Due to Python's nature it's difficult to statically analyze it, but it will catch undefined variables, basic type errors, unused code, etc. You'll want to tweak the configuration file, as by default it outputs many warnings I consider useless or harmful.\\n\\nHere's part of my .pylintrc dealing with warning silencing:\\n\\n[MESSAGES CONTROL]\\n\\n# Brain-dead errors regarding standard language features\\n#   W0142 = *args and **kwargs support\\n#   W0403 = Relative imports\\n\\n# Pointless whinging\\n#   R0201 = Method could be a function\\n#   W0212 = Accessing protected attribute of client class\\n#   W0613 = Unused argument\\n#   W0232 = Class has no __init__ method\\n#   R0903 = Too few public methods\\n#   C0301 = Line too long\\n#   R0913 = Too many arguments\\n#   C0103 = Invalid name\\n#   R0914 = Too many local variables\\n\\n# PyLint's module importation is unreliable\\n#   F0401 = Unable to import module\\n#   W0402 = Uses of a deprecated module\\n\\n# Already an error when wildcard imports are used\\n#   W0614 = Unused import from wildcard\\n\\n# Sometimes disabled depending on how bad a module is\\n#   C0111 = Missing docstring\\n\\n# Disable the message(s) with the given id(s).\\ndisable=W0142,W0403,R0201,W0212,W0613,W0232,R0903,W0614,C0111,C0301,R0913,C0103,F0401,W0402,R0914\\n\\n\"}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will create the list of dictionary in the format\n",
    "qa_openai_format = [{\"prompt\": q, \"completion\": a} for q, a in zip(questions, answers)]\n",
    "qa_openai_format[5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:27.047874Z",
     "start_time": "2023-10-12T06:52:27.036629Z"
    }
   },
   "id": "96f4882c4ed4f235"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "4429"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the training prompt\n",
    "len(qa_openai_format)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:52:28.393195Z",
     "start_time": "2023-10-12T06:52:28.367385Z"
    }
   },
   "id": "8de0f8671a219c17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Price Estimation\n",
    "\n",
    "In case you are ever worried about how many tokens your text actually has (to get an estimate of your costs) OpenAI has a library called \"tiktoken\", which allows you to estimate a cost based on token counts.\n",
    "\n",
    "Splitting text strings into tokens is useful because models like GPT-3 see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token). Different models use different encodings.\n",
    "\n",
    "**tiktoken** supports 3 different encodings for OpenAI models:\n",
    "\n",
    "* \"gpt2\" for most gpt-3 models\n",
    "* \"p50k_base\" for code models, and Davinci models, like \"text-davinci-003\"\n",
    "* \"cl100k_base\" for text-embedding-ada-002\n",
    "\n",
    "Make sure to view the pricing page on the OpenAI page for full information, for now, we'll cut down the data size so we don't spend too much money during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae5af145c1470f10"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:53:08.050810Z",
     "start_time": "2023-10-12T06:53:08.039928Z"
    }
   },
   "id": "b1b67fc8066364f3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# In order to minimize cost, we would just consider the first 500 entries from our openai format and dump it in a json file\n",
    "dataset_size = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:53:09.741469Z",
     "start_time": "2023-10-12T06:53:09.730950Z"
    }
   },
   "id": "aff752e138b02875"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# From our format, extract the same number of records\n",
    "with open(\"training_data.json\", 'w') as f:\n",
    "    for entry in qa_openai_format[:dataset_size]:\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:53:18.385351Z",
     "start_time": "2023-10-12T06:53:18.372170Z"
    }
   },
   "id": "b184e18a7c1a46e3"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Now lets find out the number of tokens\n",
    "token_counter = 0\n",
    "for element in qa_openai_format[:dataset_size]:\n",
    "    for key, value in element.items():\n",
    "        token_counter += num_tokens_from_string(value, 'p50k_base')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:54:00.563957Z",
     "start_time": "2023-10-12T06:53:57.275654Z"
    }
   },
   "id": "74be810412ea8529"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 184352 tokens\n",
      "Fine tuning using babbage costs $0.0004 per 1000 tokens\n",
      "Estimated price: $0.29496320000000004\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {token_counter} tokens\")\n",
    "print(f\"Fine tuning using babbage costs $0.0004 per 1000 tokens\")\n",
    "print(f\"Estimated price: ${(4*token_counter / 1000) * 0.0004}\") # 4 is the number of epochs we want it to train "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:54:02.962829Z",
     "start_time": "2023-10-12T06:54:02.953767Z"
    }
   },
   "id": "d26c070ce377ea16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Command Line for Fine-Tuning\n",
    "\n",
    "Note, you can find the full official guide here:\n",
    "\n",
    "https://platform.openai.com/docs/guides/fine-tuning\n",
    "\n",
    "OpenAI recommends using the terminal/command line via their OpenAI tool, which you have by simply running:\n",
    "\n",
    "    pip install --upgrade openai\n",
    "\n",
    "\n",
    "Now you can head over to the terminal to fine tune the model using the following command:\n",
    "\n",
    "    openai api fine_tunes.create -t training_data.json -m babbage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd11059d54f191c4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:54:19.449761Z",
     "start_time": "2023-10-12T06:54:19.442308Z"
    }
   },
   "id": "813bde6b9896816d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a file for fine-tuning in OpenAI from training data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f01564efa19a31"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<File file id=file-fRmeYJjtGx2eOHbZGcZBJuRH at 0x11d925610> JSON: {\n  \"object\": \"file\",\n  \"id\": \"file-fRmeYJjtGx2eOHbZGcZBJuRH\",\n  \"purpose\": \"fine-tune\",\n  \"filename\": \"file\",\n  \"bytes\": 706183,\n  \"created_at\": 1697093667,\n  \"status\": \"uploaded\",\n  \"status_details\": null\n}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will create a file for fine tuning\n",
    "openai.File.create(\n",
    "    file=open(\"training_data.json\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:54:28.035852Z",
     "start_time": "2023-10-12T06:54:24.955485Z"
    }
   },
   "id": "b4e48c42c8834a48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the fine tuned model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4242dc3d619d7f5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidRequestError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start the fine-tuning job\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFineTuningJob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfile-fRmeYJjtGx2eOHbZGcZBJuRH\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbabbage-002\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/genai/lib/python3.11/site-packages/openai/api_resources/abstract/createable_api_resource.py:57\u001B[0m, in \u001B[0;36mCreateableAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m     48\u001B[0m ):\n\u001B[1;32m     49\u001B[0m     requestor, url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__prepare_create_requestor(\n\u001B[1;32m     50\u001B[0m         api_key,\n\u001B[1;32m     51\u001B[0m         api_base,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     54\u001B[0m         organization,\n\u001B[1;32m     55\u001B[0m     )\n\u001B[0;32m---> 57\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m util\u001B[38;5;241m.\u001B[39mconvert_to_openai_object(\n\u001B[1;32m     62\u001B[0m         response,\n\u001B[1;32m     63\u001B[0m         api_key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     66\u001B[0m         plain_old_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mplain_old_data,\n\u001B[1;32m     67\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/genai/lib/python3.11/site-packages/openai/api_requestor.py:299\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    280\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    288\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    289\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    290\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    291\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    297\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    298\u001B[0m     )\n\u001B[0;32m--> 299\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/genai/lib/python3.11/site-packages/openai/api_requestor.py:710\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    702\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    704\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    705\u001B[0m         )\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[1;32m    707\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 710\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    717\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/genai/lib/python3.11/site-packages/openai/api_requestor.py:775\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    773\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 775\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    776\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    777\u001B[0m     )\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mInvalidRequestError\u001B[0m: Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview"
     ]
    }
   ],
   "source": [
    "# Start the fine-tuning job\n",
    "openai.FineTuningJob.create(training_file=\"file-fRmeYJjtGx2eOHbZGcZBJuRH\", model=\"babbage-002\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:54:56.296530Z",
     "start_time": "2023-10-12T06:54:55.536459Z"
    }
   },
   "id": "b61f15481f123ea8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we make the OpenAI call with the fined tuned model returned from the command prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cede1c5f5afa8516"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <Put down the code, after the credit card issue is resolved  >\n",
    "\n",
    "https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "\n",
    "https://norahsakal.com/blog/fine-tune-gpt3-model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac8d561c48d46807"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " response = openai.Completion.create(\n",
    "    model=\"babbage:ft-EsQwSCdusLOAWeteVTmJuMrJ\",\n",
    "    prompt=\"What are good python books?\",\n",
    "    max_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=1.0\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6d6fbb8aa921fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0db22494e4a274e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
