{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Strategies\n",
    "\n",
    "- Synchronous (Different parts of the dataset during the same time, at the end the gradients are aggregated and used to update one model)\n",
    "- Asynchronous (All workers are training at the same time and updating weights asynchronously)\n",
    "\n",
    "Training a tensorflow model on multiple GPUs, computers etc\n",
    "\n",
    "Tensorflow implement distributed Strategies in two ways\n",
    "- Mirrored Strategy\n",
    "- Multi Worker Mirrored Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing the dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped training features - (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the data\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "\n",
    "print(f\"Reshaped training features - {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training - Defining Normal Non-Distributed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normal = tf.keras.models.Sequential()\n",
    "model_normal.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))\n",
    "model_normal.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "model_normal.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "model_normal.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Distributed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "distribute = tf.distribute.MirroredStrategy()\n",
    "with distribute.scope():\n",
    "  model_distributed = tf.keras.models.Sequential()\n",
    "  model_distributed.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))\n",
    "  model_distributed.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "  model_distributed.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "  model_distributed.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Comparison between normal training and distributed training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2400/2400 [==============================] - 6s 2ms/step - loss: 0.2824 - sparse_categorical_accuracy: 0.9170\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.1374 - sparse_categorical_accuracy: 0.9587\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9694\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0855 - sparse_categorical_accuracy: 0.9730\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0716 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0640 - sparse_categorical_accuracy: 0.9794\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0581 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9839\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0477 - sparse_categorical_accuracy: 0.9842\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9854\n",
      "Normal training took: 50.70877718925476\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_normal.fit(X_train, y_train, epochs=10, batch_size=25)\n",
    "print(\"Normal training took: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2400/2400 [==============================] - 8s 3ms/step - loss: 0.2855 - sparse_categorical_accuracy: 0.9172\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.1390 - sparse_categorical_accuracy: 0.9585\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9683\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0728 - sparse_categorical_accuracy: 0.9776\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9797\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9819\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0521 - sparse_categorical_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0456 - sparse_categorical_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 7s 3ms/step - loss: 0.0431 - sparse_categorical_accuracy: 0.9860\n",
      "Distributed training took: 73.2602653503418\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_distributed.fit(X_train, y_train, epochs=10, batch_size=25)\n",
    "print(\"Distributed training took: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from above that distributed training took longer than normal training because in this case in distributed training the load was shared between GPU and CPU of the computer. Since CPU is slower than GPU , hence distributed training took longer.\n",
    "\n",
    "In cases like this where a load is distributed between GPU and CPU . it will perform slower than the entire training happening on GPU ... hence the result above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2b6a0854d3e486355aa730ef2c4309aea53da46b71110e983718c32f548066a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
