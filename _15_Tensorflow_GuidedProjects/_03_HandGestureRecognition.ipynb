{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition\n",
    "\n",
    "The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).\n",
    "\n",
    "The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2….pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation. Because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.\n",
    "\n",
    "This dataset was inspired by the Fashion-MNIST 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries Imported!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to shown an image based \n",
    "'''\n",
    "def show_img(img_number, df):\n",
    "    #  extract the label for the image number\n",
    "    label = df['label'][img_number]\n",
    "    # extract all the pixels for the image number\n",
    "    pixels = df.iloc[img_number, 1:]\n",
    "    # convert to numpy array\n",
    "    pixels = np.array(pixels, dtype='uint8')\n",
    "    pixels = pixels.reshape((28, 28))\n",
    "\n",
    "    # Now we will plot the image\n",
    "    plt.title(f\"Label is {label}\")\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0      3     107     118     127     134     139     143     146     150   \n1      6     155     157     156     156     156     157     156     158   \n2      2     187     188     188     187     187     186     187     188   \n\n   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n0     153  ...       207       207       207       207       206       206   \n1     158  ...        69       149       128        87        94       163   \n2     187  ...       202       201       200       199       198       199   \n\n   pixel781  pixel782  pixel783  pixel784  \n0       206       204       203       202  \n1       175       103       135       149  \n2       198       195       194       195  \n\n[3 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>107</td>\n      <td>118</td>\n      <td>127</td>\n      <td>134</td>\n      <td>139</td>\n      <td>143</td>\n      <td>146</td>\n      <td>150</td>\n      <td>153</td>\n      <td>...</td>\n      <td>207</td>\n      <td>207</td>\n      <td>207</td>\n      <td>207</td>\n      <td>206</td>\n      <td>206</td>\n      <td>206</td>\n      <td>204</td>\n      <td>203</td>\n      <td>202</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>155</td>\n      <td>157</td>\n      <td>156</td>\n      <td>156</td>\n      <td>156</td>\n      <td>157</td>\n      <td>156</td>\n      <td>158</td>\n      <td>158</td>\n      <td>...</td>\n      <td>69</td>\n      <td>149</td>\n      <td>128</td>\n      <td>87</td>\n      <td>94</td>\n      <td>163</td>\n      <td>175</td>\n      <td>103</td>\n      <td>135</td>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>187</td>\n      <td>188</td>\n      <td>188</td>\n      <td>187</td>\n      <td>187</td>\n      <td>186</td>\n      <td>187</td>\n      <td>188</td>\n      <td>187</td>\n      <td>...</td>\n      <td>202</td>\n      <td>201</td>\n      <td>200</td>\n      <td>199</td>\n      <td>198</td>\n      <td>199</td>\n      <td>198</td>\n      <td>195</td>\n      <td>194</td>\n      <td>195</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARENT_DIR = os.environ.get(\"ONE_DRIVE\")\n",
    "DATA_DIR = 'Datasets/GuidedProjects'\n",
    "DATA_DIR = Path(os.path.join(PARENT_DIR, DATA_DIR))\n",
    "\n",
    "data_train = pd.read_csv(DATA_DIR/'sign_mnist_train.csv')\n",
    "data_test = pd.read_csv(DATA_DIR/'sign_mnist_test.csv')\n",
    "\n",
    "# Check the data load for training dataset\n",
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0      6     149     149     150     150     150     151     151     150   \n1      5     126     128     131     132     133     134     135     135   \n2     10      85      88      92      96     105     123     135     143   \n\n   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n0     151  ...       138       148       127        89        82        96   \n1     136  ...        47       104       194       183       186       184   \n2     147  ...        68       166       242       227       230       227   \n\n   pixel781  pixel782  pixel783  pixel784  \n0       106       112       120       107  \n1       184       184       182       180  \n2       226       225       224       222  \n\n[3 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>149</td>\n      <td>149</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>151</td>\n      <td>151</td>\n      <td>150</td>\n      <td>151</td>\n      <td>...</td>\n      <td>138</td>\n      <td>148</td>\n      <td>127</td>\n      <td>89</td>\n      <td>82</td>\n      <td>96</td>\n      <td>106</td>\n      <td>112</td>\n      <td>120</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>126</td>\n      <td>128</td>\n      <td>131</td>\n      <td>132</td>\n      <td>133</td>\n      <td>134</td>\n      <td>135</td>\n      <td>135</td>\n      <td>136</td>\n      <td>...</td>\n      <td>47</td>\n      <td>104</td>\n      <td>194</td>\n      <td>183</td>\n      <td>186</td>\n      <td>184</td>\n      <td>184</td>\n      <td>184</td>\n      <td>182</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>85</td>\n      <td>88</td>\n      <td>92</td>\n      <td>96</td>\n      <td>105</td>\n      <td>123</td>\n      <td>135</td>\n      <td>143</td>\n      <td>147</td>\n      <td>...</td>\n      <td>68</td>\n      <td>166</td>\n      <td>242</td>\n      <td>227</td>\n      <td>230</td>\n      <td>227</td>\n      <td>226</td>\n      <td>225</td>\n      <td>224</td>\n      <td>222</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first three rows of the test dataset\n",
    "\n",
    "data_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now display the image number 20 from the test dataset\n",
    "show_img(20, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now display the image number 7 from the train dataset\n",
    "show_img(7, data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "\n",
    "To increase the performance of training.. but increasing the amount of data by adding slightly modified copies of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seggregate between training and testing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "y_train = data_train['label']\n",
    "y_test = data_test['label']\n",
    "\n",
    "# We will use label binarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "y_test = label_binarizer.fit_transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Extract the features\n",
    "x_train = data_train.drop(labels='label', axis=1).values\n",
    "x_test = data_test.drop(labels='label', axis=1).values\n",
    "\n",
    "# Normalize the values\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# We will do the reshaping for Neural Network Modelling\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the augmentation function and fit on training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    featurewise_center=False,  # If True, input mean will be set to 0, all features to be centered\n",
    "    samplewise_center=False, # If True, sample mean will be set to 0, all features to be centered\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    # A linear algebra operation, which reduces redundancy in images. Reduced redundancy, allows clear highlights of features\n",
    "    # Useful in cases where the images do not have clear background\n",
    "    # In our current dataset, the background is clean\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10, # Rotates the images at random between 0 - 180 degrees\n",
    "    zoom_range=0.1, # zoom into the image at random\n",
    "    width_shift_range=0.1, # This is going to shift the image horizontally and randomly\n",
    "    height_shift_range=0.1, # This is going to shift the image vertically and randomly\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "# Now we will fit the data on the training set\n",
    "data_generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 75)        750       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 75)        300       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 50)        33800     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 50)        200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 25)          11275     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 25)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                12312     \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=75, kernel_size=(3, 3), strides=1, padding='same', activation=tf.nn.relu, input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(), # Allows the neural network to train faster\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'), # will downsize the sample by taking the max of the features\n",
    "    Conv2D(filters=50, kernel_size=(3, 3), strides=1, padding='same', activation=tf.nn.relu),\n",
    "    Dropout(rate=0.2),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'),\n",
    "    Conv2D(filters=25, kernel_size=(3, 3), strides=1, padding='same', activation=tf.nn.relu),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(units=512, activation=tf.nn.relu),\n",
    "    Dropout(rate=0.3),\n",
    "    Dense(units=24, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "\n",
    "# This reduces the learning rate when a metric has stopped improving. The model performance improves by reducing the learning rate.\n",
    "# This should only be done if the learning rate stagnates\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, # learning rate should be reduced by a factor of 50%\n",
    "                                            min_lr=0.00001) # min_lr is the lower boundary for the learning rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 15:57:11.833818: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 44s 202ms/step - loss: 1.7224 - accuracy: 0.4836 - val_loss: 2.9695 - val_accuracy: 0.2167\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 43s 200ms/step - loss: 0.2485 - accuracy: 0.9164 - val_loss: 1.3139 - val_accuracy: 0.5393\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 43s 200ms/step - loss: 0.1090 - accuracy: 0.9649 - val_loss: 0.0390 - val_accuracy: 0.9961\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 43s 201ms/step - loss: 0.0667 - accuracy: 0.9786 - val_loss: 0.0161 - val_accuracy: 0.9985\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.0522 - val_accuracy: 0.9830\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.0382 - accuracy: 0.9879 - val_loss: 0.0582 - val_accuracy: 0.9769\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 0.0232 - accuracy: 0.9930 - val_loss: 0.0111 - val_accuracy: 0.9967\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 41s 190ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 46s 215ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.0094 - val_accuracy: 0.9969\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 45s 210ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0021 - val_accuracy: 0.9999\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0106 - val_accuracy: 0.9978\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 0.9999\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0016 - val_accuracy: 0.9996\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 0.9999\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 9.3361e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9997\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9997\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 7.8621e-04 - val_accuracy: 0.9997\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 7.8177e-04 - val_accuracy: 0.9999\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fc620e5d460>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(data_generator.flow(x_train, y_train, batch_size=128), epochs=20, validation_data=(x_test, y_test),\n",
    "          callbacks=[learning_rate_reduction])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 4s 19ms/step - loss: 7.8177e-04 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0007817685836926103, 0.9998605847358704]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final accuracy\n",
    "model.evaluate(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtain Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6,  5, 10,  0,  3])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will run the predictions\n",
    "predictions = model.predict_classes(x_test)\n",
    "\n",
    "for index in range(len(predictions)):\n",
    "    if predictions[index] >= 9 or predictions[index] >= 25:\n",
    "        # Both the alphabets J and Z needs hand movement and we dont have any images for them in the dataset\n",
    "        # Therefore we are incrementing the value\n",
    "        predictions[index] += 1\n",
    "\n",
    "# Check the first 5 values\n",
    "predictions[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('hand_gesture.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}